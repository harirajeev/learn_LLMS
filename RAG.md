![image](https://github.com/harirajeev/learn_LLMS/assets/13446418/0285c2c1-5897-4231-917d-b0475fff8f2e)

- [RAG CHALLENGES](https://github.com/harirajeev/learn_LLMS/blob/main/RAG%20Challenges.md)
-  [ Chunking Strategies ](https://github.com/harirajeev/learn_LLMS/blob/main/chunking%20strategies.md)
- [Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)
- [Evaluation](https://github.com/harirajeev/learn_LLMS/blob/main/Evaluation.md)
-  <b> Two stage retrieval process </b>
   -   Retrieval stage - The first stage uses embedding-based retrieval with a high top-k value to maximize recall while accepting a lower precision.
       -   Maximize recall while accepting a lower precision
       -   High recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned).
       -   Initial high-recall stage handle the variations in the ways humans speak and write,
   -   [Reranking Stage](https://www.pinecone.io/learn/series/rag/rerankers/) - Second stage uses a slightly more computationally expensive process that is higher precision and lower recall (for instance with BM25) to â€œrerankâ€ the existing retrieved candidates.
       -   Higher precision and lower recall
       -   Higher precision means that an algorithm returns more relevant results than irrelevant ones
       -   High-precision stage to accurately reflect the importance of relevance from things like phrases.
   -  ![image](https://github.com/harirajeev/learn_LLMS/assets/13446418/7022df94-6da8-4b6c-904f-b13e539da98f)

- [Maximizing LLM performance](https://blog.ankitsanghvi.in/maximizing-llm-performace/)
- [RAG Paper](https://arxiv.org/pdf/2005.11401.pdf)
- [RETRO Paper](https://arxiv.org/pdf/2112.04426.pdf)
- [Augmented Language Models: a Survey](https://arxiv.org/pdf/2302.07842.pdf)
- [How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)
- https://medium.com/@pandey.vikesh/rag-ing-success-guide-to-choose-the-right-components-for-your-rag-solution-on-aws-223b9d4c7280
- [Deploying RAG to Production](https://haystack.deepset.ai/blog/rag-deployment)
- [Why RAG is the key to safer applications with large language models](https://www.deepset.ai/blog/llms-retrieval-augmentation)
- RAG Use Case
     - ğ—¨ğ˜€ğ—²ğ—°ğ—®ğ˜€ğ—²:  A research company wants to enable their clients  ask queries against the research documents published by them. To leverage the power of longer contexts, they would have to train (not fine-tune) models on their proprietary data. This can be very pricey even with 2K. btw, ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğŸ²ğŸ±ğ—• ğ—Ÿğ—Ÿğ—®ğ— ğ—” on 100K would cost ~$150M.
     - How can we build the same claim processing app w/o expensive training:  Enter Retrieval Augmented Generation (RAG).
     - Here is the step by step:
         1. Chunk, embed, & index documents in VectorDBs (Vdb) like Weaviate or Pinecone
         2. Match the query embedding of the client.
         3. Retrieve the relevant context from Vdb.
         4. Do RAG using pre-trained LLMs such as Google's PaLM or OpenAI's GPTx.
     - ğ—ªğ—µğ˜† ğ—¥ğ—”ğ—š & ğ—˜ğ—»ğ˜ğ—²ğ—¿ğ—½ğ—¿ğ—¶ğ˜€ğ—²ğ˜€ ğ—®ğ—¿ğ—² ğ—® ğ—ºğ—®ğ˜ğ—°ğ—µ ğ—ºğ—®ğ—±ğ—² ğ—¶ğ—» ğ—µğ—²ğ—®ğ˜ƒğ—²ğ—»?
         1. Flexibility:  What if a client is not authorized to see certain documents ?You need a Role-Based Authorization Control (RBAC). It is easier to do RBAC with a RAG solution.
         2. Practicality: It is easier to index new docs in VDBs, but retraining is expensive.
         3. Broader Coverage: Only Information Extraction cases necessitate longer contexts. if you dodge that with RAG, for other uses you can use fine-tuning with a shorter context.
         4. Interpretability: Retriever knows the exact documents.
     - Embedding model that can handle longer documents?
         - Owing to self-attentionâ€™s quadratic time and space complexity there will always be a context limitation original BERT had 512.
         - But the practical problem is there is no guarantee one model will handle all document lengths. There will always be outliers in spite of you throwing a boatload of money on say 2048 context still itâ€™s not comprehensive.
         - So a better solution is to chunk documents and store them with smaller context-length models. You could save on storage costs and retrieval time.
     - When dealing with a vast corpus, beyond the embedding retrievals, a retrieval and re-ranking method is generally recommended and or maximal marginal relevance (MMR) criterion on all the document candidates.
     - [TOP MISCONCEPTIONS OF RAG](https://media.licdn.com/dms/document/media/D561FAQFslxFrkRRp9A/feedshare-document-pdf-analyzed/0/1694709823484?e=1696464000&v=beta&t=6peZ0-W0TFFpnYaz3nh6TlvHuEOyB2i1p_aLHNzJFk4)
