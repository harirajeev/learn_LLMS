The six ChatGPT risks that legal and compliance leaders should evaluate 
- Risk 1 – Fabricated and Inaccurate Answers
  - Perhaps the most common issue with ChatGPT and other LLM tools is a tendency to provide incorrect – although superficially plausible – information.
  - prone to ‘hallucinations,’ including fabricated answers that are wrong
  - Legal and compliance leaders should issue guidance that requires employees to review any output generated by ChatGPT for accuracy, appropriateness and actual usefulness before being accepted
- Risk 2 – Data Privacy and Confidentiality
- - prohibit entering sensitive organizational or personal data into public LLM tools
- Risk 3 – Model and Output Bias
- - Complete elimination of bias is likely impossible, but legal and compliance need to stay on top of laws governing AI bias, and make sure their guidance is compliant,
  - This may involve working with subject matter experts to ensure output Is reliable and with audit and technology functions to set data quality controls.
- Risk 4 – Intellectual Property (IP) and Copyright risks
- - LLMs are trained on a large amount of internet data that likely includes copyrighted material. Therefore, it’s outputs have the potential to violate copyright or IP protections
  - They does not offer source references or explanations as to how its output is generated,
  - Legal and compliance leaders should keep a keen eye on any changes to copyright law that apply to ChatGPT output and require users to scrutinize any output they generate to ensure it doesn’t infringe 
- Risk 5 – Cyber Fraud Risks
- - misusing ChatGPT to generate false information at scale (e.g., fake reviews)
  - applications that use LLM models, including ChatGPT, are also susceptible to prompt injection, a hacking technique in which malicious adversarial prompts are used to trick the model into performing tasks that it wasn’t intended
- Risk 6 – Consumer Protection Risks
  - California chatbot law mandates that in certain consumer interactions, organizations must disclose clearly and conspicuously that a consumer is communicating with a bot.

Gartner, Inc. has identified four critical areas for general counsel (GC) and legal leaders to address.

- Embed Transparency in AI Use
  - “Legal leaders need to think about how their organizations will make it clear to any humans when they are interacting with AI.”
- Ensure Risk Management Is Continuous
  -  put in place risk management controls that span the lifecycle of any high-risk AI tool,
  -  algorithmic impact assessment (AIA) that documents decision making, demonstrates due diligence, and will reduce present and future regulatory risk and other liability
- Build Governance That Includes Human Oversight and Accountability
  - human oversight which should provide internal checks on the output of AI tools.
  - designate an AI point person to help technical teams design and implement human controls
- Guard Against Data Privacy Risks
  -   privacy-by-design principles to AI initiatives. For example, require privacy impact assessments early in the project or assign privacy team members at the start to assess privacy risks.
     
