The six ChatGPT risks that legal and compliance leaders should evaluate 
- Risk 1 – Fabricated and Inaccurate Answers
  - Perhaps the most common issue with ChatGPT and other LLM tools is a tendency to provide incorrect – although superficially plausible – information.
  - prone to ‘hallucinations,’ including fabricated answers that are wrong
  - Legal and compliance leaders should issue guidance that requires employees to review any output generated by ChatGPT for accuracy, appropriateness and actual usefulness before being accepted
- Risk 2 – Data Privacy and Confidentiality
- - prohibit entering sensitive organizational or personal data into public LLM tools
- Risk 3 – Model and Output Bias
- - Complete elimination of bias is likely impossible, but legal and compliance need to stay on top of laws governing AI bias, and make sure their guidance is compliant,
  - This may involve working with subject matter experts to ensure output Is reliable and with audit and technology functions to set data quality controls.
- Risk 4 – Intellectual Property (IP) and Copyright risks
- - LLMs are trained on a large amount of internet data that likely includes copyrighted material. Therefore, it’s outputs have the potential to violate copyright or IP protections
  - They does not offer source references or explanations as to how its output is generated,
  - Legal and compliance leaders should keep a keen eye on any changes to copyright law that apply to ChatGPT output and require users to scrutinize any output they generate to ensure it doesn’t infringe 
- Risk 5 – Cyber Fraud Risks
- - misusing ChatGPT to generate false information at scale (e.g., fake reviews)
  - applications that use LLM models, including ChatGPT, are also susceptible to prompt injection, a hacking technique in which malicious adversarial prompts are used to trick the model into performing tasks that it wasn’t intended
- Risk 6 – Consumer Protection Risks
  - California chatbot law mandates that in certain consumer interactions, organizations must disclose clearly and conspicuously that a consumer is communicating with a bot.
