- Multi Head Attention (MHA)
  - [Multi-head attention mechanism: “queries”, “keys”, and “values,” over and over again](https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/)
  - [Multi-Headed Attention - code - NICE](https://nn.labml.ai/transformers/mha.html)
- Multi Query Attention (MQA)
  - [Multi-Query Attention is All You Need](https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055)
