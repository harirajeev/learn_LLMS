- Multi Head Attention (MHA)
  - [Multi-head attention mechanism: “queries”, “keys”, and “values,” over and over again](https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/)
  - [Multi-Headed Attention - code - NICE](https://nn.labml.ai/transformers/mha.html)
- Multi Query Attention (MQA)
  - [Multi-Query Attention is All You Need](https://blog.fireworks.ai/multi-query-attention-is-all-you-need-db072e758055)
- [Grouped Query Attention (GQA)](https://arxiv.org/abs/2305.13245)
  -  GQA can be regarded as a more generalized form of multi-query attention, a concept previously employed in models such as Falcon
    

![image](https://github.com/harirajeev/learn_LLMS/assets/13446418/dc665004-9f44-48b3-ad68-bcea184a3bcf)
