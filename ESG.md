serving LLMs require significant computational resources, which can lead to high energy consumption and a large carbon footprint. This can be problematic for organizations that are committed to reducing their environmental impact.

What is more shocking is that 80-90% of the machine learning workload is inference processing, according to NVIDIA. Likewise, according to AWS, inference accounts for 90% of machine learning demand in the cloud.

Just for reference, below is the image showing the financial estimation of the LLMs, along with the carbon footprint that they produce during training.

![image](https://github.com/harirajeev/learn_LLMS/assets/13446418/a3f780f4-27b9-4ace-a401-ad4e2213edc9)
