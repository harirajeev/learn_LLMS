- Perplexity. Measures how well the probability distribution predicted by the model aligns with the actual distribution of words. It is not always correlated with human judgment. Doesn’t work as well comparing models across different tasks.

- GLUE (General Language Understanding Evaluation). A collection of NLP tasks. Doesn’t

- Human evaluation.

- LLM evaluation.

- BLEU (Bilingual Evaluation Understudy). Compares n-grams in the model’s output to reference outputs. Sensitive to slight variations (only exact matches). Other variations that have improved on BLEU are ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and METEOR (Metric for Evaluation of Translation with Explicit ORdering).

- F1 Score/Precision/Recall. A classic way of measuring model quality. Evaluates the balance between precision and recall.

- Evaluate Hallucination
  - [HaluEval: A Hallucination Evaluation Benchmark for LLMs](https://github.com/RUCAIBox/HaluEval)
  - [Does your LLM application hallucinate?](https://medium.com/@saptarshichaudhuri/does-your-llm-application-hallucinate-e06eb3868a88)
  - 
