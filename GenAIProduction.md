![image](https://github.com/user-attachments/assets/f5b34532-1bae-48d5-a237-8013f8ab1e1d)


What We Learned from a Year of Building with LLMs - Eugene Yan, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu and Shreya Shankar
  - [Part 1 - tactical nuts and bolts of working with LLMs](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/)
  - [Part 2 - long-term strategic considerations](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/)
    -  [Instructor](https://github.com/jxnl/instructor) and [Outlines](https://github.com/outlines-dev/outlines) are the de facto standards for coaxing structured output from LLMs.
    -  If youâ€™re using an LLM API (e.g., Anthropic, OpenAI), use Instructor; if youâ€™re working with a self-hosted model (e.g., Hugging Face), use Outlines.
  - [Part 3](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-iii-strategy/)


Responsible AI
- [ResponsibleLLMs](https://github.com/harirajeev/learn_LLMS/blob/main/ResponsibleLLMs.md)

RAG
-  https://github.com/harirajeev/learn_LLMS/blob/main/RAG.md

LLMs can be categorised based on the number of parameters:
- ð—¦ð—ºð—®ð—¹ð—¹: â‰¤ 1 billion parameters
- ð— ð—²ð—±ð—¶ð˜‚ð—º: 1 to 10 billion parameters
- ð—Ÿð—®ð—¿ð—´ð—²: 10 to 100 billion parameters
- ð—©ð—²ð—¿ð˜† ð—Ÿð—®ð—¿ð—´ð—²: > 100 billion parameters
  
[Llama](https://github.com/harirajeev/learn_LLMS/blob/main/Llama.md)
